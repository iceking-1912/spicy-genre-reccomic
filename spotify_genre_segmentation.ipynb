import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import plotly.express as px

# Load the dataset
df = pd.read_csv('spotify_dataset.csv')

# Display the first few rows of the dataset
df.head()

# Data Pre-processing
# Check for missing values
df.isnull().sum()

# Fill or drop missing values if any (example: dropping rows with missing values)
df.dropna(inplace=True)

# Check for duplicate rows
df.duplicated().sum()

# Drop duplicate rows if any
df.drop_duplicates(inplace=True)

# Data analysis and visualizations
# Plotting histograms for numerical features
df.hist(bins=30, figsize=(15,
10))
plt.show()

# Plotting correlation matrix
correlation_matrix = df.corr()
plt.figure(figsize=(15,
10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Plotting pairplot for some features
sns.pairplot(df[
    ['feature1', 'feature2', 'feature3', 'feature4'
    ]
])
plt.show()

# Clustering
# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.select_dtypes(include=[np.number
]))

# Applying PCA for dimensionality reduction
pca = PCA(n_components=2)
pca_features = pca.fit_transform(scaled_features)

# Finding the optimal number of clusters using the elbow method
wcss = []
for i in range(1,
11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(pca_features)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,
6))
plt.plot(range(1,
11), wcss, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('Elbow Method')
plt.show()

# From the elbow plot, let's choose the optimal number of clusters, e.g.,
3
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(pca_features)

# Plotting clusters
plt.figure(figsize=(10,
6))
plt.scatter(pca_features[
    : ,
    0
], pca_features[
    : ,
    1
], c=clusters, cmap='viridis')
plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.title('Clusters of Songs')
plt.colorbar()
plt.show()

# Adding cluster information to the original dataframe
df['Cluster'
] = clusters

# Plotting clusters with plotly for better interactivity
fig = px.scatter(df, x=pca_features[
    : ,
    0
], y=pca_features[
    : ,
    1
], color=df['Cluster'
].astype(str),
                 title='Clusters of Songs', labels={'x': 'PCA Feature 1', 'y': 'PCA Feature 2'
})
fig.show()

# Model Building
# For simplicity, we can use a simple classification model to predict clusters
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Splitting the data into train and test sets
X = scaled_features
y = df['Cluster'
]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Making predictions
y_pred = model.predict(X_test)

# Evaluating the model
print(classification_report(y_test, y_pred))

# The final model can be used to recommend songs based on clusters